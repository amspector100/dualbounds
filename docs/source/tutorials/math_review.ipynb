{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a189812-274d-4e21-ac4a-e1cb4e1a0fd3",
   "metadata": {},
   "source": [
    "# Minimalist review of dual bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e918257-8594-48d5-ab78-7128c5524e2a",
   "metadata": {},
   "source": [
    "This section gives a minimal review of the original dual bounds framework from [Ji et al. (2023)](https://arxiv.org/abs/2310.08115). A more complete review can be found in the first four pages of the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2d9a0-8256-4bbb-9188-7219ae5cae52",
   "metadata": {},
   "source": [
    "Given i.i.d. outcomes $Y_i \\in \\mathcal{Y}$, treatments $W_i \\in \\{0,1\\}$ and pre-treatment covariates $X_i \\in \\mathcal{Y}$, dual bounds allow analysts to use machine learning to perform inference on partially identified estimands of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b35c0c-0917-4852-a65c-e421163c96b5",
   "metadata": {},
   "source": [
    "$$\\theta = E[f(Y_i(0), Y_i(1), X_i)], $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b2b33b-dfd3-46d1-a8a3-f47cf48698ea",
   "metadata": {},
   "source": [
    "where $Y_i(1), Y_i(0) \\in \\mathcal{Y}$ denote potential outcomes. Such estimands are *partially identified* because we never observe the joint law of the potential outcomes, but the data still contains information on the law of $(Y(0), X)$ and $(Y(1), X)$ allowing us to *bound* $\\theta$. (More generally, dual bounds can provde bounds on the solutions to generic optimization problems, but we defer discussion of this until later in the tutorial for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b1c6a-49b9-4419-a845-68d77effd39a",
   "metadata": {},
   "source": [
    "Thus, to bound $\\theta$, one must estimate the laws of $(Y(1), X)$ and $(Y(0), X)$, typically using machine learning techniques. However, it is not clear if the resulting bounds on $\\theta$ will be valid if the learned machine learning models are misspecified or inaccurate. For example, if one models $Y$ as having a linear relationship with $(X,W)$, but in truth $Y$ has a highly nonlinear relationship with $(X,W$), naive approaches can lead to inaccurate inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148683bc-e2af-49f4-8fca-69262b83e95c",
   "metadata": {},
   "source": [
    "Dual bounds are designed to leverage the benefits of sophisticated ML models without sacrificing validity. This framework has three key properties:\n",
    "\n",
    "1. **Flexibility**: Dual bounds can wrap on top of any ML model (e.g. generalized linear models, random forests, neural networks, etc).\n",
    "\n",
    "2. **Validity**: In randomized experiments, dual bounds yield provably valid confidence intervals on $\\theta$, even if the underlying ML model is arbitrarily misspecified or inaccurate. In observational studies, they have remarkably strong double robustness properties (see Section 3.4 of [Ji et al. (2023)](https://arxiv.org/abs/2310.08115)).\n",
    "\n",
    "3. **Power**: On the other hand, if the underlying ML model is highly accurate, dual bounds yield tight confidence bounds in a formal sense (see Section 3.2 of [Ji et al. (2023)](https://arxiv.org/abs/2310.08115))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39579b0b-f332-48db-8400-c2e25163ea35",
   "metadata": {},
   "source": [
    "We refer the reader to the original paper for more details on how dual bounds work. In the remaining tutorials, we show how to use dual bounds to perform inference on a variety of estimands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
