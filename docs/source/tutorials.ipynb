{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a6fbe2-9838-433c-b256-b2ab235568bd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\opt}{^{\\star}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883bcd9-def1-4275-9e73-78826287ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys; sys.path.insert(0, \"../../\")\n",
    "import numpy as np\n",
    "import dualbounds as db\n",
    "from dualbounds.generic import DualBounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a189812-274d-4e21-ac4a-e1cb4e1a0fd3",
   "metadata": {},
   "source": [
    "# Minimalist review of dual bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e918257-8594-48d5-ab78-7128c5524e2a",
   "metadata": {},
   "source": [
    "This section gives a minimal review of the dual bounds framework. A more complete review can be found in the first four pages of [Ji et al. (2023)](https://arxiv.org/abs/2310.08115). Users familiar with this content should skip this section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2d9a0-8256-4bbb-9188-7219ae5cae52",
   "metadata": {},
   "source": [
    "Given i.i.d. outcomes $Y_i \\in \\mathcal{Y}$, treatments $W_i \\in \\{0,1\\}$ and pre-treatment covariates $X_i \\in \\mathcal{Y}$, dual bounds allow analysts to use machine learning to perform inference on partially identified estimands of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b35c0c-0917-4852-a65c-e421163c96b5",
   "metadata": {},
   "source": [
    "$$\\theta = E[f(Y_i(0), Y_i(1), X_i)], $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b2b33b-dfd3-46d1-a8a3-f47cf48698ea",
   "metadata": {},
   "source": [
    "where $Y_i(1), Y_i(0) \\in \\mathcal{Y}$ denote potential outcomes. Such estimands are *partially identified* because we never observe the joint law of the potential outcomes, but the data still contains information on the law of $(Y(0), X)$ and $(Y(1), X)$ allowing us to *bound* $\\theta$. (More generally, dual bounds can provde bounds on the solutions to generic optimization problems, but we defer discussion of this until later in the tutorial for simplicity).\n",
    "\n",
    "Thus, to bound $\\theta$, one must estimate the laws of $(Y(1), X)$ and $(Y(0), X)$, typically using sophisticated machine learning techniques. However, it is not clear if the resulting bounds on $\\theta$ will be valid if the learned machine learning models are misspecified or inaccurate. For example, if one models $Y$ as having a linear relationship with $(X,W)$, but in truth $Y$ has a highly nonlinear relationship with $(X,W$), naive approaches could lead to inaccurate inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148683bc-e2af-49f4-8fca-69262b83e95c",
   "metadata": {},
   "source": [
    "Dual bounds are designed to leverage the benefits of sophisticated ML models without sacrificing validity. This framework has three key properties:\n",
    "\n",
    "1. Dual bounds can wrap on top of any ML model (e.g. generalized linear models, random forests, neural networks, etc).\n",
    "\n",
    "2. In randomized experiments, dual bounds yield provably valid confidence intervals on $\\theta$, even if the underlying ML model is arbitrarily misspecified or inaccurate. In observational studies, they have remarkably strong double robustness properties (see Section 3.4 of [Ji et al. (2023)](https://arxiv.org/abs/2310.08115)).\n",
    "\n",
    "3. On the other hand, if the underlying ML model is highly accurate, dual bounds yield tight confidence bounds in a formal sense (see Section 3.2 of [Ji et al. (2023)](https://arxiv.org/abs/2310.08115))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39579b0b-f332-48db-8400-c2e25163ea35",
   "metadata": {},
   "source": [
    "We refer the reader to the original paper for more details on how dual bounds work. In the next section, we show how to use dual bounds to perform inference on a variety of estimands "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b271090-0a44-41c9-87b6-868eccf242e8",
   "metadata": {},
   "source": [
    "# The ``DualBounds`` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139df393-4437-4d89-bbf9-250725a27cf6",
   "metadata": {},
   "source": [
    "The ``DualBounds`` class is the main class in the package. Its usage is as follows. \n",
    "\n",
    "**Step 1**: initialize the ``DualBounds`` class, which takes as an input (i) the data, (ii) the definition of the function $f$ (which defines the estimand $\\theta$), and (iii) a description of the outcome model to use as an input. The user can also input a vector of propensity scores if they are known; else they will be estimated from the data. \n",
    "\n",
    "For example, below, we show how to compute $E[\\mathbb{I}(Y(1) > Y(0)]$, the probability that the treatment effect is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f168fe92-747c-4ed9-b965-2579394c6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from a linear model\n",
    "data = db.gen_data.gen_regression_data(n=900, p=30, sample_seed=123)\n",
    "\n",
    "# Initialize dual bounds object\n",
    "dbnd = DualBounds(\n",
    "    f=lambda y0, y1, x: y0 < y1, # defines the estimand\n",
    "    covariates=data['X'], # n x p covariate matrix\n",
    "    treatment=data['W'], # n-length treatment vector\n",
    "    outcome=data['y'], # n-length outcome vector\n",
    "    propensities=data['pis'], # n-length propensity scores (optional)\n",
    "    outcome_model='ridge', # description of model for Y | X, W\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286a85b-9358-48c5-845a-9caf040b17dd",
   "metadata": {},
   "source": [
    "**Step 2**: after initialization, the ``compute_dual_bounds`` method fits the underlying outcome model and produces the final estimates and confidence bounds for the sharp partial identification bounds $\\theta_L \\le \\theta \\le \\theta_U$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a01e35-e141-419e-9370-db8ce1669fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c66165104184c12a49ded604c276f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8149bb584e114120994165a6d4a7090e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |     Lower |     Upper |\n",
      "|:-----------|----------:|----------:|\n",
      "| Estimate   | 0.6832    | 0.934563  |\n",
      "| SE         | 0.0210876 | 0.0125664 |\n",
      "| Conf. Int. | 0.641869  | 0.959193  |\n"
     ]
    }
   ],
   "source": [
    "# Compute dual bounds and observe output\n",
    "dbnd.fit(\n",
    "    nfolds=5, # number of cross-fitting folds\n",
    "    alpha=0.05, # nominal level,\n",
    "    verbose=True # show progress bars\n",
    ")\n",
    "print(dbnd.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21cc518-c07b-4122-8a14-2ed6d0865cff",
   "metadata": {},
   "source": [
    "Note that there are two estimates---a lower and an upper estimate---because $\\theta$ is not identified. One can also produce a more verbose output using the ``summary`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846b556b-b5a8-40a1-813e-9ea107673c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________Inference_____________________\n",
      "               Lower     Upper\n",
      "Estimate    0.683200  0.934563\n",
      "SE          0.021088  0.012566\n",
      "Conf. Int.  0.641869  0.959193\n",
      "\n",
      "_________________Outcome model___________________\n",
      "                      Model  No covariates\n",
      "Out-of-sample R^2  0.931781       0.000000\n",
      "RMSE               1.055246       4.040167\n",
      "MAE                0.828872       3.228010\n",
      "\n",
      "_________________Treatment model_________________\n",
      "                            Model  No covariates\n",
      "Out-of-sample R^2        0.001111       0.000000\n",
      "Accuracy                 0.500000       0.516667\n",
      "Likelihood (geom. mean)  0.500000       0.499721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbnd.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d29b2d6-111a-4fb6-a1e0-4c76250cc735",
   "metadata": {},
   "source": [
    "Another example below bounds a different estimand, the positive treatment effect $E[\\max(Y(1) - Y(0), 0)]$, using a different underlying ML model (a k-nearest neighbors regressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d26be6-6262-4334-b675-f10706ce8368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229ecfa565a54142b5b79010b3fe268b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57049af9cba243b3b2fade4d009f06dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |    Lower |    Upper |\n",
      "|:-----------|---------:|---------:|\n",
      "| Estimate   | 2.92715  | 4.305    |\n",
      "| SE         | 0.175792 | 0.151109 |\n",
      "| Conf. Int. | 2.58261  | 4.60117  |\n"
     ]
    }
   ],
   "source": [
    "dbnd = DualBounds(\n",
    "    f=lambda y0, y1, x: np.maximum(y1-y0,0), # new estimand\n",
    "    covariates=data['X'],\n",
    "    treatment=data['W'], \n",
    "    outcome=data['y'], \n",
    "    propensities=data['pis'], \n",
    "    outcome_model='knn', \n",
    ")\n",
    "dbnd.fit()\n",
    "print(dbnd.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bfceb-da1c-42b7-8603-3906f92377f3",
   "metadata": {},
   "source": [
    "# Choosing the outcome model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4c951-6807-4fce-b384-30c0fb477525",
   "metadata": {},
   "source": [
    "Dual bounds wrap on top of an underlying model which estimates the conditional distributions of $Y(1) \\mid X$ and $Y(0) \\mid X$. There are three ways to specify the underlying model, listed below in order of increasing flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145bf46-7ed7-4cd2-b64b-760f9121e114",
   "metadata": {},
   "source": [
    "## Method 1: String identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e58aaa-34c0-4039-8760-8122a81036cb",
   "metadata": {},
   "source": [
    "The easiest method is to use one of the string identifiers, such as ``'ridge', 'lasso', 'elasticnet', 'randomforest', 'knn'`` (see the API reference for a complete list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83597e1e-9dc9-41d3-97ac-a93c92bc5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnd = DualBounds(\n",
    "    f=lambda y0, y1, x: np.maximum(y1-y0,0), # estimand\n",
    "    covariates=data['X'], \n",
    "    treatment=data['W'], \n",
    "    outcome=data['y'],\n",
    "    # use a random forest to predict E[Y | X]\n",
    "    outcome_model='randomforest',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14482e72-00fa-4a31-9273-3b16185ecf6c",
   "metadata": {},
   "source": [
    "For binary data, these string identifiers assume a nonparametric model where $Y_i \\sim \\text{Bern}(\\mu(X_i, W_i))$ and the conditional mean function $\\mu$ is estimated via one of the models listed above (e.g., a random forest classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39d70e-5290-4be9-916b-50cbede13ba1",
   "metadata": {},
   "source": [
    "For nonbinary data, these string identifiers use a semiparametric regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092990ab-ca46-49af-8870-de878e2b2758",
   "metadata": {},
   "source": [
    "$$Y_i = \\mu(X_i, W_i) + \\epsilon_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323922f0-cadc-4cc8-ba1f-ccffba1b3294",
   "metadata": {},
   "source": [
    "where the conditional mean function $\\mu(\\cdot, \\cdot)$ is approximated using one of the models listed above (e.g., a random forest or k-nearest neighbors regressor). All methods automatically create interaction terms between the covariates and the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23c21d-3097-4147-af6b-8b06b175ca33",
   "metadata": {},
   "source": [
    "**Default 1: Homoskedasticity.** By default, these string identifiers estimate a homoskedastic model where the variance of $\\epsilon_i$ does not depend on $X_i$. However, one can also specify a model to use to estimate the heteroskedasticity pattern, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d615561-7d8a-46dd-bab9-d0d8d0f28367",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnd = DualBounds(\n",
    "    f=lambda y0, y1, x: np.maximum(y1-y0,0), # estimand\n",
    "    covariates=data['X'], \n",
    "    treatment=data['W'], \n",
    "    outcome=data['y'],\n",
    "    # use a random forest to predict E[Y | X]\n",
    "    outcome_model='randomforest', \n",
    "    # use lasso to predict Var(Y | X)\n",
    "    heterosked_model='lasso',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209752d4-8467-4dcf-85af-c323a9a22fd3",
   "metadata": {},
   "source": [
    "That said, we emphasize that the default (homoskedastic) approach yields valid bounds even under arbitrary heteroskedasticity patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b00e8-7a05-43f0-aef0-87119e8b932c",
   "metadata": {},
   "source": [
    "**Default 2: Nonparametric residual estimates.** By default, these string identifiers estimate the law of $\\epsilon_i$ using the empirical law of the training residuals (or, for ridge estimators, the leave-one-out residuals). However, it is possible to change this by changing the ``eps_dist`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a77d62b-6049-40b5-8adb-154c069da190",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnd = DualBounds(\n",
    "    f=lambda y0, y1, x: np.maximum(y1-y0,0), # estimand\n",
    "    covariates=data['X'], \n",
    "    treatment=data['W'], \n",
    "    outcome=data['y'],\n",
    "    propensities=data['pis'],\n",
    "    # use a random forest to predict E[Y | X]\n",
    "    outcome_model='randomforest',\n",
    "    # assume a parametric model for the residuals\n",
    "    # (the default is nonparametric)\n",
    "    eps_dist='laplace', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f898a-a6f1-404c-b0de-4820bbda0dd9",
   "metadata": {},
   "source": [
    "## Method 2: A ``dist_reg.DistReg`` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1435d-cf8a-4c11-a12b-bf28f1eae938",
   "metadata": {},
   "source": [
    "Analysts can also specify the outcome model by passing in a model which inherits from ``dualbounds.dist_reg.DistReg``, including the ``CtsDistReg``, ``QuantileDistReg``, or and ``BinaryDistReg`` classes in the ``dualbounds.dist_reg`` submodule. One example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6c50ae-2806-4df4-8138-b1fd93e4a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8b5dab93eb405fb795dbad29d8e702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd11d26be2a4b9aaf3de9cbf274d942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |   Lower |     Upper |\n",
      "|:-----------|--------:|----------:|\n",
      "| Estimate   | 3.08487 | 3.29493   |\n",
      "| SE         | 0.10184 | 0.0951055 |\n",
      "| Conf. Int. | 2.88527 | 3.48134   |\n"
     ]
    }
   ],
   "source": [
    "Y_model = db.dist_reg.CtsDistReg(\n",
    "    model_type='elasticnet', \n",
    "    eps_dist='empirical',\n",
    "    how_transform='interactions', # create interactions btwn X and W\n",
    "    heterosked_model='lasso',\n",
    "    heterosked_kwargs=dict(cv=3), # kwargs for model for Var(Y|X)\n",
    ")\n",
    "dbnd = DualBounds(\n",
    "    outcome_model=Y_model, # use new model\n",
    "    f=lambda y0, y1, x: np.maximum(y1-y0,0), # estimand\n",
    "    covariates=data['X'], \n",
    "    treatment=data['W'], \n",
    "    outcome=data['y'],\n",
    "    propensities=data['pis'],\n",
    ")\n",
    "dbnd.fit(alpha=0.05)\n",
    "print(dbnd.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da62a40-daad-4d38-a912-f6331dfac790",
   "metadata": {},
   "source": [
    "One can also directly input ``sklearn`` or ``sklearn``-like classes. For example, below we show how to use the ``AdaBoostClassifier`` from sklearn for binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d263885-febd-445a-84d2-b0ab98def55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1ad2ddb12e4b41bf280877ff59b2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506285ba1f824f608546c93ac63cf4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |     Lower |     Upper |\n",
      "|:-----------|----------:|----------:|\n",
      "| Estimate   | 0.294189  | 0.397205  |\n",
      "| SE         | 0.0284652 | 0.0226536 |\n",
      "| Conf. Int. | 0.238398  | 0.441606  |\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble as ensemble\n",
    "Y_model = db.dist_reg.BinaryDistReg(\n",
    "    model_type=ensemble.AdaBoostClassifier,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "dbnd = DualBounds(\n",
    "    outcome_model=Y_model, # use new model\n",
    "    f=lambda y0, y1, x: y0 < y1, # estimand\n",
    "    outcome=data['y'] > 0, # make the outcome binary\n",
    "    # other data\n",
    "    treatment=data['W'], \n",
    "    covariates=data['X'],\n",
    "    propensities=data['pis'],\n",
    ")\n",
    "dbnd.fit(alpha=0.05)\n",
    "print(dbnd.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc525c-8e8a-44e7-89b5-eae4d3da4810",
   "metadata": {},
   "source": [
    "Analysts can also create custom classes inheritting from ``dualbounds.dist_reg.DistReg``, allowing analysts to use (e.g.) custom conditional variance estimators---see the API reference for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a85ff-860c-4fab-9804-a746dd11070e",
   "metadata": {},
   "source": [
    "## Method 3: Input predicted conditional distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a44187-ad1a-48f2-b1a9-78218c5d2eb8",
   "metadata": {},
   "source": [
    "For maximum flexibility, one can also directly input predicted conditional distributions of $Y(1) \\mid X$ and $Y(0) \\mid X$, in the form of a list of batched scipy distributions whose shapes sum to the number of datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf854b53-67f1-4a9a-bb09-33969b4b9196",
   "metadata": {},
   "source": [
    "This is illustrated below, although for simplicity the inputs have nothing to do with the true distributions of $Y(1) \\mid X$ and $Y(0) \\mid X$. Note that in real applications, it is extremely important that the estimates of $Y(1) \\mid X$ and $Y(0) \\mid X$ must be computed using cross-fitting, otherwise the dual bounds may not be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3243ace7-4952-4b31-b4e0-4acd71d1ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da20113c54d44eaba695ab9a8b7020bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |     Lower |     Upper |\n",
      "|:-----------|----------:|----------:|\n",
      "| Estimate   | 0.318794  | 1.2159    |\n",
      "| SE         | 0.0391768 | 0.0261528 |\n",
      "| Conf. Int. | 0.242008  | 1.26716   |\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "n = len(data['y']) # number of data-points\n",
    "\n",
    "# Initialize object\n",
    "dbnd = DualBounds(\n",
    "    Y_model='lasso', # this will be ignored\n",
    "    f=lambda y0, y1, x : y0 < y1, # estimand\n",
    "    # data\n",
    "    outcome=data['y'],\n",
    "    treatment=data['W'], \n",
    "    covariates=data['X'],\n",
    "    propensities=data['pis'],\n",
    ")\n",
    "\n",
    "# Either of the following input formats work\n",
    "y0_dists = stats.norm(loc=np.zeros(n))\n",
    "y1_dists = [\n",
    "    stats.norm(loc=np.zeros(int(n/2)), scale=2), \n",
    "    stats.norm(loc=np.zeros(int(n/2)), scale=3)\n",
    "]\n",
    "# Compute dual bounds using y0_dists and y1_dists\n",
    "dbnd.fit(\n",
    "    y0_dists=y0_dists,\n",
    "    y1_dists=y1_dists,\n",
    "    suppress_warning=True,\n",
    ")\n",
    "print(dbnd.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3262d-b6c9-42b7-b6c0-13da8b1912b7",
   "metadata": {},
   "source": [
    "This syntax can be useful if in simulations one wants to compute an \"oracle dual bound\" which has perfect knowledge of the conditional distributions of $Y(0) \\mid X$ and $Y(1) \\mid X$, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae34c28-476e-4d0e-95ef-133b1cc60ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7646cce800924bc3859be112a0eaa05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |     Lower |     Upper |\n",
      "|:-----------|----------:|----------:|\n",
      "| Estimate   | 0.675722  | 0.929035  |\n",
      "| SE         | 0.0208299 | 0.0126281 |\n",
      "| Conf. Int. | 0.634896  | 0.953786  |\n"
     ]
    }
   ],
   "source": [
    "# Compute oracle dual bounds using the true conditional dists of Y0/Y1\n",
    "dbnd.fit(\n",
    "    y0_dists=data['y0_dists'],\n",
    "    y1_dists=data['y1_dists'],\n",
    "    suppress_warning=True,\n",
    ")\n",
    "print(dbnd.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370dde1-2801-4f8f-b1a0-c9c3b8720534",
   "metadata": {},
   "source": [
    "Note that the output of the oracle dual bounds is extremely similar to the output of the initial dual bounds in the third cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f763a72-8a66-4cce-ac1f-1939bf9d2e05",
   "metadata": {},
   "source": [
    "# Choosing the propensity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408b84c-b8fc-4e51-bf0d-3a33cbe21d85",
   "metadata": {},
   "source": [
    "Dual bounds can also apply to observational data where the propensity scores must be estimated. In this case, analysts can specify the model used to estimate the propensity scores---the ``propensity_model``---with one of three methods. First, one can use a string identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca956b23-4a69-4ff4-95dc-19e34d62012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting propensity scores.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a9128f3c5a45b992ea662825fe1595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5173fb79394932b08c0be38f47465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895931cbf23c40e79118ff081533ffb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________Inference_____________________\n",
      "               Lower     Upper\n",
      "Estimate    0.692290  0.931634\n",
      "SE          0.020940  0.013118\n",
      "Conf. Int.  0.651249  0.957345\n",
      "\n",
      "_________________Outcome model___________________\n",
      "                      Model  No covariates\n",
      "Out-of-sample R^2  0.931776       0.000000\n",
      "RMSE               1.055278       4.040167\n",
      "MAE                0.827924       3.228010\n",
      "\n",
      "_________________Treatment model_________________\n",
      "                            Model  No covariates\n",
      "Out-of-sample R^2        0.007721       0.000000\n",
      "Accuracy                 0.535556       0.516667\n",
      "Likelihood (geom. mean)  0.501630       0.499721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbnd = DualBounds(\n",
    "    propensity_model='ridge', # logistic ridge for prop. scores\n",
    "    outcome_model='lasso',\n",
    "    f=lambda y0, y1, x: y0 < y1, # estimand\n",
    "    outcome=data['y'],\n",
    "    treatment=data['W'], \n",
    "    covariates=data['X'],\n",
    ")\n",
    "dbnd.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ec004-4f93-4f64-899e-90ca1e08ca42",
   "metadata": {},
   "source": [
    "Second, one can directly input an sklearn classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34e63ca-8785-460a-8ec2-f1b7123298a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting propensity scores.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888633bbdbe440698222ec28f16141e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e35295eabea4fbc8e0a8ff70bb52972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de45848cd724a1e9dc56f245d38a350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________Inference_____________________\n",
      "               Lower     Upper\n",
      "Estimate    0.685896  0.933438\n",
      "SE          0.021124  0.012638\n",
      "Conf. Int.  0.644494  0.958209\n",
      "\n",
      "_________________Outcome model___________________\n",
      "                      Model  No covariates\n",
      "Out-of-sample R^2  0.931781       0.000000\n",
      "RMSE               1.055246       4.040167\n",
      "MAE                0.828872       3.228010\n",
      "\n",
      "_________________Treatment model_________________\n",
      "                            Model  No covariates\n",
      "Out-of-sample R^2       -0.014126       0.000000\n",
      "Accuracy                 0.501111       0.516667\n",
      "Likelihood (geom. mean)  0.496089       0.499721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbnd = DualBounds(\n",
    "    propensity_model=ensemble.AdaBoostClassifier(algorithm='SAMME'), \n",
    "    f=lambda y0, y1, x: y0 < y1, # estimand\n",
    "    outcome=data['y'],\n",
    "    treatment=data['W'], \n",
    "    covariates=data['X']\n",
    ")\n",
    "dbnd.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cdbb00-40b2-4f62-9df4-23bcceb70169",
   "metadata": {},
   "source": [
    "Lastly, analysts can also directly estimate the vector propensity scores and input them, although analysts should ensure that they are correctly employing cross-fitting in this case to ensure validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1328bc4-eb46-442a-be4f-ce187d77e47c",
   "metadata": {},
   "source": [
    "# Model selection with the multiplier bootstrap (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea1b4a-e5b5-4e36-91bf-c76694d0e38a",
   "metadata": {},
   "source": [
    "In randomized experiments, the ``DualBounds`` object produces valid confidence intervals under arbitrary misspecification of the outcome model, meaning that the analyst can try many different outcome models to see which produces the tightest bounds. After fitting many outcome models and computing dual bounds based on each of them, the multiplier bootstrap provides a principled way to combine evidence across many different estimates while retaining rigorous coverage guarantees for the final confidence interval (i.e., accounting for multiplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22e4ac-7c4c-4b81-874e-1a48f6edd6b9",
   "metadata": {},
   "source": [
    "Below, we show how to do this in challenging setting with heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3960e0c-8c17-42f4-9cd5-3491dae927e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from a linear model with heteroskedasticity\n",
    "data = db.gen_data.gen_regression_data(\n",
    "    n=900, # Num. datapoints\n",
    "    p=30, # Num. covariates\n",
    "    tauv=0.2,\n",
    "    interactions=True, # ensures treatment effect is heterogenous\n",
    "    heterosked='exp_linear', # Pattern of heteroskedasticity\n",
    "    eps_dist='laplace',\n",
    "    sample_seed=123, # random seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1a282-c8de-455c-a344-4a0d163d29db",
   "metadata": {},
   "source": [
    "First, we compute dual bounds based on several different choices of outcome model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba70447d-541f-44e9-984d-519eb56f4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting outcome_model=ridge, heterosked_model=none.\n",
      "Fitting outcome_model=ridge, heterosked_model=lasso.\n",
      "Fitting outcome_model=ridge, heterosked_model=rf.\n",
      "Fitting outcome_model=rf, heterosked_model=none.\n",
      "Fitting outcome_model=rf, heterosked_model=lasso.\n",
      "Fitting outcome_model=rf, heterosked_model=rf.\n"
     ]
    }
   ],
   "source": [
    "# Nominal level\n",
    "alpha = 0.1\n",
    "# List of dualbounds objects\n",
    "db_objects = []\n",
    "widths = []\n",
    "Y_models = ['ridge', 'rf']\n",
    "heterosked_models = ['none', 'lasso', 'rf']\n",
    "model_names = []\n",
    "for Y_model in Y_models:\n",
    "    for heterosked_model in heterosked_models:\n",
    "        print(f\"Fitting outcome_model={Y_model}, heterosked_model={heterosked_model}.\")\n",
    "        gdb = DualBounds(\n",
    "            # data\n",
    "            outcome=data['y'],\n",
    "            treatment=data['W'], \n",
    "            covariates=data['X'],\n",
    "            propensities=data['pis'],\n",
    "            # estimand\n",
    "            f=lambda y0, y1, x: y0 < y1,\n",
    "            # models\n",
    "            outcome_model=Y_model,\n",
    "            heterosked_model=heterosked_model,\n",
    "            eps_dist='laplace',\n",
    "        )\n",
    "        gdb.fit(nfolds=3, alpha=alpha, verbose=False)\n",
    "        db_objects.append(gdb)\n",
    "        widths.append(gdb.cis[1] - gdb.cis[0])\n",
    "        model_names.append(f'Y_model={Y_model}\\nHeterosked={heterosked_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738929ce-6df5-47b0-a7ac-116083374a82",
   "metadata": {},
   "source": [
    "Second, we aggregate the bounds using the multiplier bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801374ca-cbec-4b6c-90f7-d6b39cf9042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           |    Lower |    Upper |\n",
      "|:----------|---------:|---------:|\n",
      "| Estimate  | 0.68564  | 0.86214  |\n",
      "| Conf. Int | 0.637019 | 0.897106 |\n"
     ]
    }
   ],
   "source": [
    "bootstrap_output = db.bootstrap.dualbound_multiplier_bootstrap(\n",
    "    db_objects, alpha=alpha\n",
    ")\n",
    "print(bootstrap_output.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0784cd-feea-4fe9-82cc-bafcbdddda17",
   "metadata": {},
   "source": [
    "We can see that the bootstrap output width is nearly equal to the smallest width among all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e9116e-f090-4084-a6ad-c9798fa8ddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bootstrap_width = bootstrap_output.values[1, 1] \n",
    "bootstrap_width -= bootstrap_output.values[1, 0]\n",
    "ax.scatter(model_names, widths, color='black')\n",
    "ax.axhline(\n",
    "    bootstrap_width, \n",
    "    label='Width of Multiplier Boostrap Confidence Interval', \n",
    "    color='red',\n",
    "    linestyle='dashed',\n",
    ")\n",
    "ax.set(ylabel=\"Width of confidence interval\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0b296-ea71-419a-9e30-22fc58436d41",
   "metadata": {},
   "source": [
    "# Additional estimands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf32591-9c92-49be-9c7c-0c5eaf42f1cd",
   "metadata": {},
   "source": [
    "Dual bounds can apply beyond the settings described in the previous sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df63cd-0995-4597-abc0-bffc36fab31d",
   "metadata": {},
   "source": [
    "## Variance of the CATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dade1f9-529f-4eb5-9611-abd16e9dc470",
   "metadata": {},
   "source": [
    "Dual bounds can also be used to *lower-bound* the variance of the conditional average treatment effect $\\theta = \\text{Var}(E[Y(1) - Y(0) \\mid X])$, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd122531-a280-4d0c-9eb6-d8a1ec963661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8433cddfd544edb83cea25180e789f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |    Lower |   Upper |\n",
      "|:-----------|---------:|--------:|\n",
      "| Estimate   | 7.20519  |     nan |\n",
      "| SE         | 0.528092 |     nan |\n",
      "| Conf. Int. | 6.33655  |     nan |\n"
     ]
    }
   ],
   "source": [
    "vdb = db.varcate.VarCATEDualBounds(\n",
    "    outcome=data['y'],\n",
    "    treatment=data['W'], \n",
    "    covariates=data['X'],\n",
    "    propensities=data['pis'],\n",
    "    outcome_model='elasticnet',\n",
    ")\n",
    "vdb.fit()\n",
    "print(vdb.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a5cb7-e805-4554-adf8-d62a1486ed9c",
   "metadata": {},
   "source": [
    "## Variance of the ITE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6ebf4-75cd-4f36-8097-0ccc321d4abf",
   "metadata": {},
   "source": [
    "Dual bounds can also be used to upper and lower bound the variance of the individual treatment effect $\\theta = \\text{Var}(Y(1) - Y(0))$, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9372eca-9280-478b-b9c6-f5992cc0b232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2065fb6e030346af8ecc576be4b41759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b7b4cb6145498abe8f01167af4bb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |    Lower |     Upper |\n",
      "|:-----------|---------:|----------:|\n",
      "| Estimate   | 8.74237  | 11.7923   |\n",
      "| SE         | 0.575671 |  0.676127 |\n",
      "| Conf. Int. | 7.61408  | 13.1174   |\n"
     ]
    }
   ],
   "source": [
    "vdb = db.varite.VarITEDualBounds(\n",
    "    outcome=data['y'],\n",
    "    treatment=data['W'], \n",
    "    covariates=data['X'],\n",
    "    propensities=data['pis'],\n",
    "    outcome_model='elasticnet',\n",
    ")\n",
    "vdb.fit()\n",
    "print(vdb.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5c08e-67fa-4abf-bb03-f4e49b59608f",
   "metadata": {},
   "source": [
    "## Generic extensions via the delta method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641405a-1659-4d2a-8c3c-6d2529b7d1a8",
   "metadata": {},
   "source": [
    "Using the delta method, dual bounds can also be used to upper and lower bound estimands of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a8f3e-ae68-4eb5-8711-d162120536fc",
   "metadata": {},
   "source": [
    "$$\\theta = h(E[f(Y(0), Y(1), X)], E[z_1(Y(1), X)], E[z_0(Y(0), X)]) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504da88-dcdc-4c4c-b6f7-ab1572f121fd",
   "metadata": {},
   "source": [
    "where $h$ is a continuous function that is nondecreasing in its first input, $f$ is a real-valued function, and $z_1$ and $z_0$ are potentially vector-valued functions. Using the ``dualbounds.delta.DeltaDualBounds`` class, one can directly input these functions to produce dual bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3c2981c-5966-4a92-804e-7bc251623572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95bcf36402a48f493facf72d8412171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66331453a524572bb2171c41271b80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            |    Lower |    Upper |\n",
      "|:-----------|---------:|---------:|\n",
      "| Estimate   | 0.394742 | 0.403239 |\n",
      "| SE         | 0.100678 | 0.101925 |\n",
      "| Conf. Int. | 0.197417 | 0.603009 |\n"
     ]
    }
   ],
   "source": [
    "delta_db = db.delta.DeltaDualBounds(\n",
    "    # input arbitrary functions\n",
    "    h=lambda fval, z1, z0: fval / z0 + z1[0] * z1[1],\n",
    "    z1=lambda y1, x: np.array([y1, x[0]]),\n",
    "    z0=lambda y0, x: y0**2,\n",
    "    f=lambda y0, y1, x: np.maximum(y1, y0),\n",
    "    # input data\n",
    "    outcome=data['y'],\n",
    "    treatment=data['W'], \n",
    "    covariates=data['X'],\n",
    "    propensities=data['pis'],\n",
    ")\n",
    "delta_db.fit()\n",
    "print(delta_db.results().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35510998-5a1c-4130-a1f5-0047c58d047d",
   "metadata": {},
   "source": [
    "Indeed, the above code yields bounds on the estimand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4528773-f21e-418c-80ff-e8295da330ca",
   "metadata": {},
   "source": [
    "$$\\theta = \\frac{E[(Y(1)-Y(0))_+]}{E[Y(0)^2]} + E[Y(1)] \\cdot E[X_1].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadce15d-3484-46d2-980f-ce9afa381445",
   "metadata": {},
   "source": [
    "## Lee Bounds under monotonicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf0c25-664b-49f1-b61a-5c6654904dee",
   "metadata": {},
   "source": [
    "Lee bounds are a method to bound the average treatment effect in the face of post-treatment nonrandom sample selection, named in honor of  [Lee (2009)](https://www.jstor.org/stable/40247633). Precisely, we assume we observe the following data:\n",
    "\n",
    "- Pre-treatment covariates $X_i \\in \\mathcal{X}$\n",
    "\n",
    "- A binary treatment $W_i \\in \\{0,1\\}$\n",
    " \n",
    "- A post-treatment selection indicator $S_i \\in \\{0,1\\}$.\n",
    " \n",
    "- An outcome $Y_i \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce6a98-26f7-4adf-84a4-bdf7b5c5c254",
   "metadata": {},
   "source": [
    "Note that both $Y_i$ and $S_i$ have potential outcomes $(Y_i(0), Y_i(1))$ and $(S_i(0), S_i(1))$ since both potentially depend on the treatment.\n",
    "\n",
    "A classic example is a setting where $W_i$ denotes enrollment in a job training program, $S_i$ denotes whether a subject entered the labor market, and the outcome $Y_i$ denotes wages. A natural estimand in these settings is the average treatment effect for subjects who would have entered the labor market no matter their treatment status; e.g., "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165992c-a024-4eb7-b255-8442f5046c30",
   "metadata": {},
   "source": [
    "$$E[Y(1) - Y(0) \\mid S(1) = S(0) = 1]. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e5b51-27da-4e55-938b-fcaa76debb94",
   "metadata": {},
   "source": [
    "Dual bounds can be used to bound this partially identified estimand under the **monotonicity** assumption that $S(1) \\ge S(0)$ a.s., as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbcb8ec0-c90c-4ff1-bb32-5a34f10a86fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the selection model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d7cd3c38114c249ce491c6d408a55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-fitting the outcome model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcc1c6d9bf34dafa991d5782d74f6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating optimal dual variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175414c4039e481e95ae53c7d9871b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________Inference_____________________\n",
      "               Lower     Upper\n",
      "Estimate    2.220620  3.155182\n",
      "SE          0.191205  0.193274\n",
      "Conf. Int.  1.845866  3.533992\n",
      "\n",
      "________________Selection model__________________\n",
      "                            Model  No covariates\n",
      "Out-of-sample R^2        0.172740       0.000000\n",
      "Accuracy                 0.703333       0.594444\n",
      "Likelihood (geom. mean)  0.557705       0.508488\n",
      "\n",
      "_________________Outcome model___________________\n",
      "                      Model  No covariates\n",
      "Out-of-sample R^2  0.919644       0.000000\n",
      "RMSE               1.057244       3.729639\n",
      "MAE                0.846142       2.954590\n",
      "\n",
      "________________Treatment model__________________\n",
      "                            Model  No covariates\n",
      "Out-of-sample R^2        0.001111       0.000000\n",
      "Accuracy                 0.500000       0.516667\n",
      "Likelihood (geom. mean)  0.500000       0.499721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create data\n",
    "lee_data = db.gen_data.gen_lee_bound_data(n=900, p=30, sample_seed=123)\n",
    "\n",
    "# fit lee bounds\n",
    "ldb = db.lee.LeeDualBounds(\n",
    "    # data\n",
    "    selections=lee_data['S'], \n",
    "    covariates=lee_data['X'], \n",
    "    treatment=lee_data['W'],\n",
    "    propensities=lee_data['pis'], \n",
    "    outcome=lee_data['y'],\n",
    "    # Model specifications\n",
    "    outcome_model='ridge',\n",
    "    selection_model='monotone_logistic',\n",
    ")\n",
    "ldb.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88378eef-dc0f-47d8-97fb-94b7a67987ed",
   "metadata": {},
   "source": [
    "It is also possible to bound this estimand without the monotonicity assumption using the generic ``DualBounds`` class, although we caution that without the monotonicity assumption, the bounds might be too wide to be useful. Please see [Ji et al. (2023)](https://arxiv.org/pdf/2310.08115.pdf), Section 2.5 for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
